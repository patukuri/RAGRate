RAGRate is a comprehensive automated evaluation framework designed to assess the performance
of Retrieval-Augmented Generation (RAG) models, which combine language models with external
knowledge retrieval capabilities. RAG models have gained significant traction in recent years, as they
enable language models to leverage external information sources, leading to more reliable and
factual responses.

RAGRate compares the answers from the LLM application aganist multiple open sources Large Language Models and returns the evaluation metrics for each of the model.
The evaluation metrics that RAGRate measures are 
1. Accuracy
2. Context Relevance
3. Fiathfulness

Detailed report if attached as a additional file. Please check summary file for more details.
